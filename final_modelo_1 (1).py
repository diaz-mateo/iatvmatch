# -*- coding: utf-8 -*-
"""FINAL MODELO 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFKxEpqatMU0MzqSqAFiu_GkYbBFUGJU
"""

!pip install gpt4all

!pip install streamlit

import os
import subprocess

modelo_path = "Meta-Llama-3-8B-Instruct.Q4_0.gguf"

if not os.path.exists(modelo_path):
    print("Descargando el modelo...")
    subprocess.run([
        "wget",
        "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_0.gguf",
        "-O", modelo_path
    ])
else:
    print("El modelo ya está descargado.")

import json
from gpt4all import GPT4All

import subprocess

subprocess.run([
    "wget",
    "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_0.gguf",
    "-O", "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
])

def generar_recomendaciones(modelo, prompt, max_tokens=512, temperature=0.7):
    """
    Función que utiliza GPT4All para generar recomendaciones a partir de un prompt.

    Parámetros:
      - modelo: Modelo cargado de GPT4All.
      - prompt: Instrucción completa para el modelo.
      - max_tokens: Número máximo de tokens a generar.
      - temperature: Controla la aleatoriedad en la generación.

    Retorna:
      - respuesta: La respuesta generada en texto.
    """
    try:
        with modelo.chat_session():
            # Se usa 'temp' en lugar de 'temperature' para que coincida con la firma de la función generate.
            respuesta = modelo.generate(prompt, max_tokens=max_tokens, temp=temperature)
        return respuesta
    except Exception as e:
        print(f"Error durante la generación: {e}")
        return None

def procesar_respuesta(respuesta_json):
    """
    Función para procesar la respuesta generada en formato JSON y extraer información clave.
    Si la respuesta no es un JSON válido, se muestra el contenido generado en bruto.
    """
    try:
        datos = json.loads(respuesta_json)
        recomendaciones = datos.get("recomendaciones", [])
        if not recomendaciones:
            print("No se encontraron recomendaciones en la respuesta.")
        else:
            for idx, serie in enumerate(recomendaciones, start=1):
                print(f"\nRecomendación {idx}:")
                print(f"  Título: {serie.get('título', 'Desconocido')}")
                print(f"  Descripción: {serie.get('descripción', 'Sin descripción')}")
                print(f"  Fecha: {serie.get('fecha', 'No disponible')}")
                elenco = serie.get('elenco', [])
                if isinstance(elenco, list):
                    print(f"  Elenco: {', '.join(elenco)}")
                else:
                    print(f"  Elenco: {elenco}")
    except json.JSONDecodeError:
        print("No se pudo procesar la respuesta como JSON. Respuesta generada:")
        print(respuesta_json)

def main():
    # Cargar el modelo. Asegúrate de que el archivo "Meta-Llama-3-8B-Instruct.Q4_0.gguf" se haya descargado correctamente.
    modelo = cargar_modelo()
    if modelo is None:
        return

    # Solicitar al usuario el nombre o descripción de la serie.
    serie_usuario = input("Introduce el nombre o descripción de una serie: ").strip()

    # Construir el prompt con instrucciones claras para la IA.
    prompt = f"""
Dado el nombre de una serie de TV estadounidense de comedia o sitcom, proporciona una lista de al menos cinco series similares en cuanto a tono, temática y estilo de humor.
Para cada recomendación, incluye:
    - "título": Nombre de la serie.
    - "descripción": Breve sinopsis de la serie.
    - "fecha": Año de lanzamiento.
    - "elenco": Lista de actores destacados.
Justifica brevemente por qué cada serie es una buena alternativa, considerando elementos como el reparto, la narrativa y la audiencia objetivo.
Genera la respuesta en formato JSON con la siguiente estructura:
{{
    "recomendaciones": [
        {{
            "título": "Nombre de la serie",
            "descripción": "Breve sinopsis",
            "fecha": "Año de lanzamiento",
            "elenco": ["Actor 1", "Actor 2"]
        }},
        ...
    ]
}}
Serie de referencia: "{serie_usuario}"
    """

    print("Generando recomendaciones, por favor espere...")
    respuesta = generar_recomendaciones(modelo, prompt)

    if respuesta:
        print("\nRespuesta completa del modelo:")
        print(respuesta)
        print("\nProcesando la respuesta...")
        procesar_respuesta(respuesta)
    else:
        print("No se obtuvo respuesta del modelo.")

if __name__ == "__main__":
    main()
